{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "71LUix8zGsAn",
    "outputId": "ff711a6c-77bf-4a06-c37c-ec578c1032a7"
   },
   "outputs": [],
   "source": [
    "!pip install pyspark\n",
    "!pip install pyngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dfcJg5meJoeo",
    "outputId": "5b9d3714-5c35-4048-99d8-a0c03f4d07cc"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"CropYieldAnalysis\").config(\"spark.ui.port\",\"4050\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vxPUxY0Vvw8T",
    "outputId": "6274699b-1f17-4bba-a6ad-c4963c38b78e"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the CSV file\n",
    "df = spark.read.csv(\"uncleaned_crop_yield.csv\", header=True, inferSchema=False)\n",
    "# Display initial data\n",
    "print(\"Initial Data:\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VoTnyC_Gwk0s",
    "outputId": "453e0606-9db4-4413-bd23-786c5e41aeb1"
   },
   "outputs": [],
   "source": [
    "# Drop duplicate rows\n",
    "df = df.dropDuplicates()\n",
    "\n",
    "# Display schema and information about the DataFrame\n",
    "print(\"DataFrame Info:\")\n",
    "df.printSchema()\n",
    "\n",
    "# Show a summary of the DataFrame (similar to .info() in Pandas)\n",
    "print(\"Data Summary:\")\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8HzsdTz4xwat",
    "outputId": "81843e14-dd13-478c-ce02-0b26b36a84d4"
   },
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "# Fill numeric columns with their mean\n",
    "from pyspark.sql.functions import col\n",
    "numeric_columns = [col_name for col_name, dtype in df.dtypes if dtype in (\"int\", \"double\")]\n",
    "for col_name in numeric_columns:\n",
    "    mean_value = df.select(col_name).groupBy().avg(col_name).first()[0]\n",
    "    df = df.fillna({col_name: mean_value})\n",
    "\n",
    "# Fill categorical columns with a placeholder\n",
    "categorical_columns = [col_name for col_name, dtype in df.dtypes if dtype == \"string\"]\n",
    "df = df.fillna({col_name: \"Unknown\" for col_name in categorical_columns})\n",
    "\n",
    "# Verify missing values\n",
    "print(\"Missing values after filling:\")\n",
    "df.select([col(c).isNull().alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XSAsbemXXtSc",
    "outputId": "bde7c75a-5a7f-45b8-8789-83833c74c7e1"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import trim\n",
    "\n",
    "# Strip whitespace from string columns\n",
    "for col_name in categorical_columns:\n",
    "    df = df.withColumn(col_name, trim(col(col_name)))\n",
    "\n",
    "# Display the DataFrame to verify the result\n",
    "print(\"DataFrame after trimming whitespace from string columns:\")\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DuB_OpUWZIoG"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jvC5N6dnZi31",
    "outputId": "3f92723b-0b18-4000-99f2-5d2188417069"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, upper, isnan\n",
    "\n",
    "# Assume `df` is the PySpark DataFrame\n",
    "\n",
    "# Convert all string columns to uppercase\n",
    "string_columns = [col_name for col_name, dtype in df.dtypes if dtype == \"string\"]\n",
    "for col_name in string_columns:\n",
    "    df = df.withColumn(col_name, upper(col(col_name)))\n",
    "\n",
    "# Display a sample of the data after converting strings to uppercase\n",
    "print(\"Sample data after converting strings to uppercase:\")\n",
    "df.show(5, truncate=False)\n",
    "\n",
    "# Count null values for all columns after the transformation\n",
    "null_counts_after_upper = (\n",
    "    df.select([when(col(c).isNull() | isnan(c), 1).alias(c) for c in df.columns])\n",
    "    .groupBy()\n",
    "    .sum()\n",
    ")\n",
    "\n",
    "print(\"Null counts per column after converting strings to uppercase:\")\n",
    "null_counts_after_upper.show()\n",
    "\n",
    "# Count the number of rows after the transformation\n",
    "row_count_after_upper = df.count()\n",
    "print(f\"Number of rows after converting strings to uppercase: {row_count_after_upper}\")\n",
    "\n",
    "# Collect and display a sample of the processed data\n",
    "processed_data_after_upper = df.take(5)  # Collect the first 5 rows\n",
    "for row in processed_data_after_upper:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lGQAf7dygJ-Y",
    "outputId": "64e544a3-e457-47d9-d2d8-d30226cef032"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Convert 'Region' column to StringType (already default for strings in PySpark)\n",
    "df = df.withColumn(\"Region\", col(\"Region\").cast(StringType()))\n",
    "\n",
    "# Convert 'Crop' column to StringType\n",
    "df = df.withColumn(\"Crop\", col(\"Crop\").cast(StringType()))\n",
    "\n",
    "# Display schema to confirm the datatype\n",
    "df.printSchema()\n",
    "\n",
    "# Show the updated DataFrame\n",
    "df.select(\"Region\", \"Crop\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BX1J5eDmisdr",
    "outputId": "1541c040-ea46-44cd-97e6-10487936769d"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import round\n",
    "\n",
    "# Round the 'Rainfall_mm' column to 2 decimal places\n",
    "df = df.withColumn(\"Rainfall_mm\", round(col(\"Rainfall_mm\"), 2))\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NYtf7V9ylbse",
    "outputId": "a9d7fbca-f815-4521-ecaf-337f5e1b06f6"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Identify numeric columns\n",
    "numeric_columns = [col_name for col_name, dtype in df.dtypes if dtype in (\"int\", \"double\")]\n",
    "\n",
    "# Loop through numeric columns to filter outliers\n",
    "for col_name in numeric_columns:\n",
    "    # Calculate Q1 and Q3 using approxQuantile\n",
    "    Q1, Q3 = df.approxQuantile(col_name, [0.25, 0.75], relativeError=0.01)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Filter rows within the bounds\n",
    "    df = df.filter((col(col_name) >= lower_bound) & (col(col_name) <= upper_bound))\n",
    "\n",
    "# Show the DataFrame information\n",
    "print(\"Filtered DataFrame:\")\n",
    "df.printSchema()\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oHfE4D_ypHrH",
    "outputId": "0e2dc5a3-f851-401a-b46f-da22aa5dda85"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "# Define categorical columns to encode\n",
    "categorical_columns = ['Region', 'Soil_Type', 'Crop', 'Weather_Condition', 'Irrigation_Used', 'Fertilizer_Used']\n",
    "\n",
    "# Step 1: Remove existing index/encoded columns if present\n",
    "columns_to_remove = [f\"{col}_Index\" for col in categorical_columns] + [f\"{col}_Encoded\" for col in categorical_columns]\n",
    "df = df.drop(*[c for c in columns_to_remove if c in df.columns])\n",
    "\n",
    "# Step 2: Apply StringIndexer and OneHotEncoder\n",
    "indexers = [StringIndexer(inputCol=col_name, outputCol=f\"{col_name}_Index\") for col_name in categorical_columns]\n",
    "encoders = [OneHotEncoder(inputCol=f\"{col_name}_Index\", outputCol=f\"{col_name}_Encoded\", dropLast=False) for col_name in categorical_columns]\n",
    "\n",
    "# Combine into a pipeline\n",
    "pipeline = Pipeline(stages=indexers + encoders)\n",
    "\n",
    "# Fit and transform the DataFrame\n",
    "pipeline_model = pipeline.fit(df)\n",
    "encoded_df = pipeline_model.transform(df)\n",
    "\n",
    "# Step 3: Convert OneHotEncoded vectors to separate binary columns\n",
    "def extract_dense_vector(vector, index):\n",
    "    return float(vector[index]) if vector is not None else 0.0\n",
    "\n",
    "for col_name in categorical_columns:\n",
    "    num_categories = encoded_df.select(f\"{col_name}_Encoded\").first()[0].size  # Get number of categories\n",
    "    for i in range(num_categories):\n",
    "        extract_udf = udf(lambda vector: extract_dense_vector(vector, i), FloatType())\n",
    "        encoded_df = encoded_df.withColumn(f\"{col_name}_{i}\", extract_udf(col(f\"{col_name}_Encoded\")))\n",
    "\n",
    "# Step 4: Drop original, indexed, and encoded columns\n",
    "columns_to_drop = categorical_columns + [f\"{col}_Index\" for col in categorical_columns] + [f\"{col}_Encoded\" for col in categorical_columns]\n",
    "encoded_df = encoded_df.drop(*columns_to_drop)\n",
    "\n",
    "# Show the final DataFrame\n",
    "encoded_df.show(truncate=False)\n",
    "encoded_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NSadY9oRdgrk"
   },
   "outputs": [],
   "source": [
    "# Define the columns to scale\n",
    "columns_to_convert = [ \"Days_to_Harvest\", \"Yield_tons_per_hectare\"]\n",
    "\n",
    "# Step 1: Replace null values with 0.0 and cast to double\n",
    "for col_name in columns_to_convert:\n",
    "    encoded_df = encoded_df.withColumn(col_name,\n",
    "                                       when(col(col_name).isNull(), 0.0).otherwise(col(col_name).cast(\"double\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CvYQUSuhtycW",
    "outputId": "d95eded4-605c-47ad-a3d2-092a8ca48eca"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler, VectorAssembler\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Define the columns to scale\n",
    "columns_to_scale = [\"Rainfall_mm\", \"Temperature_Celsius\"]\n",
    "\n",
    "# Step 1: Replace null values with 0.0 and cast to double\n",
    "for col_name in columns_to_scale:\n",
    "    encoded_df = encoded_df.withColumn(col_name,\n",
    "                                       when(col(col_name).isNull(), 0.0).otherwise(col(col_name).cast(\"double\")))\n",
    "\n",
    "# Step 2: Create a VectorAssembler to combine the columns into a single vector\n",
    "assembler = VectorAssembler(inputCols=columns_to_scale, outputCol=\"features_to_scale\")\n",
    "\n",
    "# Transform the DataFrame to add the 'features_to_scale' column\n",
    "df = assembler.transform(encoded_df)\n",
    "\n",
    "# Step 3: Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler(inputCol=\"features_to_scale\", outputCol=\"scaled_features\")\n",
    "\n",
    "# Step 4: Fit and transform the scaler to scale the features\n",
    "scaler_model = scaler.fit(df)\n",
    "scaled_df = scaler_model.transform(df)\n",
    "\n",
    "# Step 5: Convert the vector column to an array to extract individual elements\n",
    "scaled_df = scaled_df.withColumn(\"scaled_features_array\", vector_to_array(col(\"scaled_features\")))\n",
    "\n",
    "# Step 6: Extract the scaled features back into individual columns\n",
    "for i, col_name in enumerate(columns_to_scale):\n",
    "    scaled_df = scaled_df.withColumn(col_name + \"_scaled\", col(\"scaled_features_array\")[i])\n",
    "\n",
    "# Step 7: Drop intermediate columns (optional)\n",
    "scaled_df = scaled_df.drop(\"features_to_scale\", \"scaled_features\", \"scaled_features_array\", \"Rainfall_mm\", \"Temperature_Celsius\")\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "scaled_df.show(truncate=False)\n",
    "\n",
    "# Print schema to verify new scaled columns\n",
    "scaled_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P7MZFsAlg2zU",
    "outputId": "0f796ab9-a4c6-4904-b0a9-1a1610eb20ac"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_spec = Window.partitionBy(\"Region_0\").orderBy(F.desc(\"Yield_tons_per_hectare\"))\n",
    "df = scaled_df.withColumn(\"Rank\", F.rank().over(window_spec))\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cmdCIRxfvQwl"
   },
   "source": [
    "Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ooPDn-6DvPxc",
    "outputId": "e241d4e2-dcee-48b0-c26d-edc669d99efe"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Separate features (X) by dropping the target column\n",
    "X = scaled_df.drop(\"Yield_tons_per_hectare\")\n",
    "\n",
    "# Select the target column (Y)\n",
    "Y = scaled_df.select(\"Yield_tons_per_hectare\")\n",
    "\n",
    "# Show summary statistics for features\n",
    "print(\"Feature Summary:\")\n",
    "X.describe().show()\n",
    "\n",
    "# Show summary statistics for the target variable\n",
    "print(\"Target Summary:\")\n",
    "Y.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hu6M_r011u_I"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "# Step 1: Combine features into a single vector column\n",
    "feature_columns = [col for col in scaled_df.columns if col != \"Yield_tons_per_hectare\"]\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "dataset = assembler.transform(scaled_df).select(\"features\", col(\"Yield_tons_per_hectare\").alias(\"label\"))\n",
    "\n",
    "# Step 2: Split the dataset into training and testing sets (with features and label together)\n",
    "train_df, test_df = dataset.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Step 3: Scale the features\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withMean=True, withStd=True)\n",
    "scaler_model = scaler.fit(train_df)\n",
    "train_df = scaler_model.transform(train_df).select(\"scaled_features\", \"label\")\n",
    "test_df = scaler_model.transform(test_df).select(\"scaled_features\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "63L5jrFIa4Qi",
    "outputId": "106d83af-be26-4874-fd22-7dbae2668dd4"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "linear_reg = LinearRegression(featuresCol=\"scaled_features\", labelCol=\"label\", predictionCol=\"prediction\")\n",
    "linear_model = linear_reg.fit(train_df)\n",
    "\n",
    "# Step 5: Make predictions on the test data\n",
    "predictions = linear_model.transform(test_df)\n",
    "\n",
    "# Step 6: Evaluate the model\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "mse = evaluator.evaluate(predictions, {evaluator.metricName: \"mse\"})\n",
    "r2 = evaluator.evaluate(predictions, {evaluator.metricName: \"r2\"})\n",
    "\n",
    "# Output results\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"R Squared (R²): {r2}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
