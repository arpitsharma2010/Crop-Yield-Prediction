{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e357b65",
   "metadata": {},
   "source": [
    "# Updated Project Notebook for PySpark Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943bce32",
   "metadata": {},
   "source": [
    "## 1. Setting up PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec65312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uncleaned_crop_yield.csv MapPartitionsRDD[25] at textFile at NativeMethodAccessorImpl.java:0\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 1 times, most recent failure: Lost task 0.0 in stage 8.0 (TID 8) (Darshan executor driver): java.net.SocketException: Connection reset by peer\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write0(Native Method)\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:54)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.tryWrite(NioSocketImpl.java:394)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implWrite(NioSocketImpl.java:413)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.write(NioSocketImpl.java:440)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$2.write(NioSocketImpl.java:819)\r\n\tat java.base/java.net.Socket$SocketOutputStream.write(Socket.java:1195)\r\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:125)\r\n\tat java.base/java.io.BufferedOutputStream.implWrite(BufferedOutputStream.java:222)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:200)\r\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:115)\r\n\tat java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:110)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeUTF(PythonRDD.scala:492)\r\n\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:312)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.net.SocketException: Connection reset by peer\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write0(Native Method)\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:54)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.tryWrite(NioSocketImpl.java:394)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implWrite(NioSocketImpl.java:413)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.write(NioSocketImpl.java:440)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$2.write(NioSocketImpl.java:819)\r\n\tat java.base/java.net.Socket$SocketOutputStream.write(Socket.java:1195)\r\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:125)\r\n\tat java.base/java.io.BufferedOutputStream.implWrite(BufferedOutputStream.java:222)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:200)\r\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:115)\r\n\tat java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:110)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeUTF(PythonRDD.scala:492)\r\n\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:312)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(rdd)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Extract header\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m header \u001b[38;5;241m=\u001b[39m \u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfirst\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m data_rdd \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mfilter(\u001b[38;5;28;01mlambda\u001b[39;00m row: row \u001b[38;5;241m!=\u001b[39m header)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Split rows into columns\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\DARSHAN SONAWANE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\rdd.py:2888\u001b[0m, in \u001b[0;36mRDD.first\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2862\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfirst\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[T]\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m   2863\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2864\u001b[0m \u001b[38;5;124;03m    Return the first element in this RDD.\u001b[39;00m\n\u001b[0;32m   2865\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2886\u001b[0m \u001b[38;5;124;03m    ValueError: RDD is empty\u001b[39;00m\n\u001b[0;32m   2887\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2888\u001b[0m     rs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2889\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rs:\n\u001b[0;32m   2890\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m rs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\DARSHAN SONAWANE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\rdd.py:2855\u001b[0m, in \u001b[0;36mRDD.take\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   2852\u001b[0m         taken \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2854\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(partsScanned, \u001b[38;5;28mmin\u001b[39m(partsScanned \u001b[38;5;241m+\u001b[39m numPartsToTry, totalParts))\n\u001b[1;32m-> 2855\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeUpToNumLeft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2857\u001b[0m items \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m res\n\u001b[0;32m   2858\u001b[0m partsScanned \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m numPartsToTry\n",
      "File \u001b[1;32mc:\\Users\\DARSHAN SONAWANE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\context.py:2510\u001b[0m, in \u001b[0;36mSparkContext.runJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m   2508\u001b[0m mappedRDD \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmapPartitions(partitionFunc)\n\u001b[0;32m   2509\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2510\u001b[0m sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmappedRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartitions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2511\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, mappedRDD\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[1;32mc:\\Users\\DARSHAN SONAWANE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\DARSHAN SONAWANE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\DARSHAN SONAWANE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 1 times, most recent failure: Lost task 0.0 in stage 8.0 (TID 8) (Darshan executor driver): java.net.SocketException: Connection reset by peer\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write0(Native Method)\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:54)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.tryWrite(NioSocketImpl.java:394)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implWrite(NioSocketImpl.java:413)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.write(NioSocketImpl.java:440)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$2.write(NioSocketImpl.java:819)\r\n\tat java.base/java.net.Socket$SocketOutputStream.write(Socket.java:1195)\r\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:125)\r\n\tat java.base/java.io.BufferedOutputStream.implWrite(BufferedOutputStream.java:222)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:200)\r\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:115)\r\n\tat java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:110)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeUTF(PythonRDD.scala:492)\r\n\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:312)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.net.SocketException: Connection reset by peer\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write0(Native Method)\r\n\tat java.base/sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:54)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.tryWrite(NioSocketImpl.java:394)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implWrite(NioSocketImpl.java:413)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.write(NioSocketImpl.java:440)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$2.write(NioSocketImpl.java:819)\r\n\tat java.base/java.net.Socket$SocketOutputStream.write(Socket.java:1195)\r\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:125)\r\n\tat java.base/java.io.BufferedOutputStream.implWrite(BufferedOutputStream.java:222)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:200)\r\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:115)\r\n\tat java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:110)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeUTF(PythonRDD.scala:492)\r\n\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:312)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:322)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:322)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:751)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"CropYieldAnalysis\").getOrCreate()\n",
    "\n",
    "# Load the CSV file as RDD\n",
    "rdd = spark.sparkContext.textFile(r\"uncleaned_crop_yield.csv\")\n",
    "\n",
    "# Extract header\n",
    "header = rdd.first()\n",
    "data_rdd = rdd.filter(lambda row: row != header)\n",
    "\n",
    "# Split rows into columns\n",
    "split_rdd = data_rdd.map(lambda row: row.split(\",\"))\n",
    "\n",
    "# Infer schema based on the number of columns in the header\n",
    "column_names = header.split(\",\")\n",
    "num_columns = len(column_names)\n",
    "\n",
    "# Check RDD structure\n",
    "print(f\"Number of columns: {num_columns}\")\n",
    "print(f\"Column names: {column_names}\")\n",
    "print(\"Sample data:\")\n",
    "for row in split_rdd.take(5):  # Show the first 5 rows\n",
    "    print(row)\n",
    "\n",
    "# Optional: Convert the RDD back to a DataFrame for further processing\n",
    "from pyspark.sql import Row\n",
    "df = split_rdd.map(lambda cols: Row(**{column_names[i]: cols[i] for i in range(len(cols))})).toDF()\n",
    "\n",
    "# Show the schema and data\n",
    "df.printSchema()\n",
    "df.show(5)\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67ec7b2",
   "metadata": {},
   "source": [
    "## 2. Loading Data into PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a8cbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into an RDD\n",
    "data_path = \"uncleaned_crop_yield.csv\"  # Update with your dataset path\n",
    "rdd = sc.textFile(data_path)\n",
    "\n",
    "# Remove the header (assuming the first row is the header)\n",
    "header = rdd.first()  # Extract the header\n",
    "rdd = rdd.filter(lambda row: row != header)  # Filter out the header row\n",
    "\n",
    "# Split each row into columns (assuming CSV is comma-separated)\n",
    "rdd = rdd.map(lambda row: row.split(\",\"))\n",
    "\n",
    "# Display the schema (column names from the header)\n",
    "print(\"Schema:\", header.split(\",\"))\n",
    "\n",
    "# Show the first 5 rows of the RDD\n",
    "print(\"Sample data:\")\n",
    "for row in rdd.take(5):\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e527d7a8",
   "metadata": {},
   "source": [
    "## 3. Distributed Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae34d0b",
   "metadata": {},
   "source": [
    "Removing Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6f62ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = rdd.distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81867bf9",
   "metadata": {},
   "source": [
    "Replace Data Inplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7706386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate means for the columns\n",
    "rainfall_mean = rdd.map(lambda row: float(row[0]) if row[0] else None).filter(lambda x: x is not None).mean()\n",
    "temperature_mean = rdd.map(lambda row: float(row[1]) if row[1] else None).filter(lambda x: x is not None).mean()\n",
    "days_to_harvest_mean = rdd.map(lambda row: float(row[2]) if row[2] else None).filter(lambda x: x is not None).mean()\n",
    "\n",
    "# Fill missing values with the column means\n",
    "def fill_missing_values(row):\n",
    "    return [\n",
    "        float(row[0]) if row[0] else rainfall_mean,  # Rainfall_mm\n",
    "        float(row[1]) if row[1] else temperature_mean,  # Temperature_Celsius\n",
    "        float(row[2]) if row[2] else days_to_harvest_mean,  # Days_to_Harvest\n",
    "        float(row[3]) if row[3] else None  # Yield_tons_per_hectare\n",
    "    ]\n",
    "\n",
    "rdd_filled = rdd.map(fill_missing_values)\n",
    "\n",
    "# Filter out rows where Yield_tons_per_hectare is still None\n",
    "rdd_cleaned = rdd_filled.filter(lambda row: row[3] is not None)\n",
    "\n",
    "# Display the number of missing values in each column\n",
    "null_counts = rdd_cleaned.map(lambda row: [\n",
    "    row[i] is None for i in range(len(row))\n",
    "]).reduce(lambda acc, row: [acc[i] + row[i] for i in range(len(row))])\n",
    "\n",
    "print(\"Null counts per column:\", null_counts)\n",
    "\n",
    "# Show first 5 rows of the cleaned RDD\n",
    "print(\"Cleaned data sample:\")\n",
    "for row in rdd_cleaned.take(5):\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fe9d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the mean for a specific column index\n",
    "def compute_mean_for_index(rdd, idx):\n",
    "    # Extract numeric values and filter out None or empty values\n",
    "    column_values = rdd.map(lambda row: float(row[idx]) if row[idx] not in [None, \"\", \"null\"] else None) \\\n",
    "                       .filter(lambda x: x is not None)\n",
    "\n",
    "    # Debug: Check if column_values is empty\n",
    "    if column_values.isEmpty():\n",
    "        print(f\"Column {idx} is empty. Returning 0 as default mean.\")\n",
    "        return 0  # Return 0 or a fallback mean if the column is empty\n",
    "\n",
    "    # Compute total sum and count\n",
    "    try:\n",
    "        total_sum = column_values.reduce(lambda x, y: x + y)\n",
    "        count = column_values.count()\n",
    "        return total_sum / count if count > 0 else 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing mean for column {idx}: {e}\")\n",
    "        return 0\n",
    "\n",
    "# Function to replace null values in a column with the mean\n",
    "def fill_missing_with_mean(row, numeric_indices, means):\n",
    "    for i, idx in enumerate(numeric_indices):\n",
    "        if row[idx] in [None, \"\", \"null\"]:  # Check for missing or null values\n",
    "            row[idx] = means[i]  # Replace with the computed mean\n",
    "    return row\n",
    "\n",
    "# Example workflow\n",
    "\n",
    "# Step 1: Specify numeric column indices\n",
    "numeric_indices = [3, 4, 7]  # Replace with the actual column indices for 'Rainfall_mm', 'Temperature_Celsius', 'Days_to_Harvest'\n",
    "\n",
    "# Step 2: Compute means for the specified numeric columns\n",
    "means = [compute_mean_for_index(rdd, idx) for idx in numeric_indices]\n",
    "print(\"Computed means for numeric columns:\", means)\n",
    "\n",
    "# Step 3: Replace null values with the computed mean for each column\n",
    "rdd = rdd.map(lambda row: fill_missing_with_mean(row, numeric_indices, means))\n",
    "\n",
    "# Debug: Check cleaned rows\n",
    "print(\"Sample cleaned data:\")\n",
    "for row in rdd.take(5):\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7f2f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify string columns\n",
    "string_cols = [field.name for field in df.schema.fields if isinstance(field.dataType, StringType)]\n",
    "\n",
    "# Apply the trim function to remove leading and trailing whitespace from string columns\n",
    "for column in string_cols:\n",
    "    df = df.withColumn(column, trim(col(column)))\n",
    "\n",
    "# Display the transformed DataFrame\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7254e053",
   "metadata": {},
   "source": [
    " 4. Consistency in columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46faa16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Replace values in 'Fertilizer_Used'\n",
    "df = df.withColumn(\"Fertilizer_Used\", \n",
    "                   when(col(\"Fertilizer_Used\") == False, 'F')\n",
    "                   .when(col(\"Fertilizer_Used\") == True, 'T')\n",
    "                   .otherwise(col(\"Fertilizer_Used\").cast(\"string\")))\n",
    "\n",
    "# Replace values in 'Irrigation_Used'\n",
    "df = df.withColumn(\"Irrigation_Used\", \n",
    "                   when(col(\"Irrigation_Used\") == False, 'F')\n",
    "                   .when(col(\"Irrigation_Used\") == True, 'T')\n",
    "                   .otherwise(col(\"Irrigation_Used\").cast(\"string\")))\n",
    "\n",
    "# Display the unique values to verify\n",
    "print(\"Unique values in Fertilizer_Used after replacement:\")\n",
    "df.select(\"Fertilizer_Used\").distinct().show()\n",
    "\n",
    "print(\"Unique values in Irrigation_Used after replacement:\")\n",
    "df.select(\"Irrigation_Used\").distinct().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1472fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Identify string columns\n",
    "string_cols = [field.name for field in df.schema.fields if isinstance(field.dataType, StringType)]\n",
    "\n",
    "# Apply the transformation: convert all string columns to uppercase\n",
    "for column in string_cols:\n",
    "    df = df.withColumn(column, upper(col(column)))\n",
    "\n",
    "# Show the transformed DataFrame\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6980140",
   "metadata": {},
   "source": [
    "Convert String to Capitalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f79745c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, upper\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Identify string columns\n",
    "string_cols = [field.name for field in df.schema.fields if isinstance(field.dataType, StringType)]\n",
    "\n",
    "# Apply the upper function to convert string columns to uppercase\n",
    "for column in string_cols:\n",
    "    df = df.withColumn(column, upper(col(column)))\n",
    "\n",
    "# Display the transformed DataFrame\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abf67ac",
   "metadata": {},
   "source": [
    "Rounding decimal places to 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb94fa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import round\n",
    "\n",
    "# Round the 'Rainfall_mm' column to 2 decimal places\n",
    "df = df.withColumn(\"Rainfall_mm\", round(col(\"Rainfall_mm\"), 2))\n",
    "\n",
    "# Show the transformed DataFrame\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5c0679",
   "metadata": {},
   "source": [
    "Outliers removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe73fff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Identify numeric columns\n",
    "numeric_cols = [field.name for field in df.schema.fields if str(field.dataType) in ['DoubleType', 'IntegerType']]\n",
    "\n",
    "# Compute and remove outliers for each numeric column\n",
    "for col_name in numeric_cols:\n",
    "    # Calculate Q1 and Q3 using approxQuantile\n",
    "    Q1, Q3 = df.approxQuantile(col_name, [0.25, 0.75], 0.01)  # 0.01 is the relative error\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Filter rows within the bounds\n",
    "    df = df.filter((col(col_name) >= lower_bound) & (col(col_name) <= upper_bound))\n",
    "\n",
    "# Show the updated DataFrame schema and sample rows\n",
    "df.printSchema()\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a0d07c",
   "metadata": {},
   "source": [
    "One-hot-encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb77836",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Columns to be encoded\n",
    "categorical_cols = ['Region', 'Soil_Type', 'Crop', 'Weather_Condition', 'Irrigation_Used', 'Fertilizer_Used']\n",
    "\n",
    "# Index and encode each categorical column\n",
    "for col_name in categorical_cols:\n",
    "    # Convert strings to indices\n",
    "    indexer = StringIndexer(inputCol=col_name, outputCol=f\"{col_name}_Index\")\n",
    "    df = indexer.fit(df).transform(df)\n",
    "    \n",
    "    # One-hot encode the indexed values\n",
    "    encoder = OneHotEncoder(inputCol=f\"{col_name}_Index\", outputCol=f\"{col_name}_Vec\")\n",
    "    df = encoder.fit(df).transform(df)\n",
    "\n",
    "# Show the transformed DataFrame with encoded columns\n",
    "df.select(*[f\"{col}_Vec\" for col in categorical_cols]).show(5)\n",
    "\n",
    "# Print schema to verify\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0967e62c",
   "metadata": {},
   "source": [
    "Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde8ebfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Combine the columns to be scaled into a single feature vector\n",
    "assembler = VectorAssembler(inputCols=[\"Rainfall_mm\", \"Temperature_Celsius\"], outputCol=\"features\")\n",
    "df = assembler.transform(df)\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "\n",
    "# Fit the scaler to the data and transform it\n",
    "scaler_model = scaler.fit(df)\n",
    "df = scaler_model.transform(df)\n",
    "\n",
    "# Extract scaled values into separate columns if needed\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "# UDF to extract individual values from scaled features\n",
    "extract_first = udf(lambda vec: float(vec[0]), FloatType())\n",
    "extract_second = udf(lambda vec: float(vec[1]), FloatType())\n",
    "\n",
    "df = df.withColumn(\"Scaled_Rainfall_mm\", extract_first(col(\"scaled_features\")))\n",
    "df = df.withColumn(\"Scaled_Temperature_Celsius\", extract_second(col(\"scaled_features\")))\n",
    "\n",
    "# Drop intermediate columns if not needed\n",
    "df = df.drop(\"features\", \"scaled_features\")\n",
    "\n",
    "# Show the transformed DataFrame\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecd98fd",
   "metadata": {},
   "source": [
    "## 6. Analyzing Performance with DAG Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2e47e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The DAG visualization is available on the Spark UI during execution.\n",
    "# Navigate to http://localhost:4040 (or your Spark server's UI) to view job and stage DAGs for performance analysis.\n",
    "print(\"Visit the Spark UI to view DAGs and execution details.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
